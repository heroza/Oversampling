{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Oversampling/blob/main/Skin_Cancer_Diagnosis_using_ADASYN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "16de3d33-930b-4dc7-c4a9-f93dfd04f198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfcFpsBwM0d4"
      },
      "source": [
        "#Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C_s6OIGKM26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "371bd24a-4bf9-491a-e0ec-78b7eb06e6d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ff488085aaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_current_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_preprocessing_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Image preprocessing layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m \u001b[0mcython_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'core'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tk\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,AveragePooling2D,BatchNormalization,Add,ZeroPadding2D,Flatten,Dense,Input,LeakyReLU,Softmax,ReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class Attention(tk.layers.Layer):\n",
        "    \n",
        "    def __init__(self,input_channels,output_channel,kernel_size,groups):\n",
        "        super().__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channel = output_channel    \n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = 1\n",
        "        self.groups = groups\n",
        "\n",
        "        assert output_channel % groups == 0\n",
        "        \n",
        "        self.rel_h = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,kernel_size,1,output_channel//2),stddev = 0.1)) \n",
        "        #output_channels//2 is the number of channels on which the relative position will be considered,1 denotes the number of those filters and the one after that too and (kernel_size,1) denotes the size of that filter\n",
        "        self.rel_w = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,1,kernel_size,output_channel//2),stddev = 0.1)) \n",
        "\n",
        "        self.key_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.query_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.value_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "\n",
        "    def call(self,x):\n",
        "        \n",
        "        batch,height,width,channels = x.shape\n",
        "        x_padded = ZeroPadding2D(padding=(self.kernel_size//2,self.kernel_size//2))(x)\n",
        "        query = self.query_weights(x)\n",
        "        value = self.value_weights(x_padded)\n",
        "        key = self.key_weights(x_padded)\n",
        "        #key,query and value will have the shape of (batch,height,width,depth)\n",
        "        keys = tf.image.extract_patches(images = key,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        value = tf.image.extract_patches(images = value,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        no_of_kernels = key.shape[-2] - self.kernel_size + 1\n",
        "        keys = tf.reshape(keys,shape = (-1,no_of_kernels, no_of_kernels,self.kernel_size,self.kernel_size,self.output_channel))\n",
        "        key_split_h,key_split_w = tf.split(keys,num_or_size_splits = 2,axis = -1)\n",
        "        key_with_rel = tk.layers.concatenate([key_split_h + self.rel_h,key_split_w + self.rel_w],axis = -1) \n",
        "        \n",
        "        #reshaping the query and key\n",
        "        key_with_rel = tf.reshape(key_with_rel,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        query  = tf.reshape(query,(-1,self.groups,no_of_kernels,no_of_kernels,1,self.output_channel//self.groups))        \n",
        "        value = tf.reshape(value,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        \n",
        "        #multiplication  of key and query\n",
        "        #assert key_with_rel.shape == query.shape        \n",
        "        key_prod_query = query*key_with_rel\n",
        "        \n",
        "        # Now the function is passed through the softmax and is multiplied with the values\n",
        "        s = Softmax(axis = -2)(key_prod_query)\n",
        "        y = tf.einsum('bnchwk,bnchwk->bnchk',s,value)\n",
        "        y = tf.reshape(y,(-1,height,width,self.output_channel))\n",
        "        return y\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            \"input_channels\": self.input_channels, \n",
        "            \"output_channel\": self.output_channel, \n",
        "            \"kernel_size\": self.kernel_size, \n",
        "            \"stride\": self.stride, \n",
        "            \"groups\": self.groups, \n",
        "            \"rel_h\": self.rel_h, \n",
        "            \"rel_w\": self.rel_w, \n",
        "            \"key_weights\": self.key_weights, \n",
        "            \"query_weights\": self.query_weights, \n",
        "            \"value_weights\": self.value_weights\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_x4c0_DTkaa"
      },
      "source": [
        "#Library, atribut, and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR2MJBYq-oiB",
        "outputId": "c6083363-915d-4ec2-9860-1adc55c28c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 224\n",
        "IMAGE_H = 224\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)\n",
        "num_classes = 7\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "opt_adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "opt_SGD = SGD(learning_rate=0.001)\n",
        "the_arch = 'resnet50'\n",
        "\n",
        "#Data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2), \n",
        "  layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),\n",
        "  layers.experimental.preprocessing.RandomTranslation(0.3, 0.3, fill_mode='reflect', interpolation='bilinear',)\n",
        "])\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model_attention.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/last_model_attention.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_balanced_acc', patience=20, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_balanced_acc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JffFid9sOXeo"
      },
      "outputs": [],
      "source": [
        "# load train and test dataset\n",
        "def preprocess_image_input(input_images, arch = the_arch):\n",
        "  input_images = input_images.astype('float32')\n",
        "  if arch == 'inception_v3':\n",
        "    output_ims = tf.keras.applications.inception_v3.preprocess_input(input_images)\n",
        "  else:\n",
        "    output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def load_cifar10_dataset():\n",
        "  from keras.datasets import cifar10\n",
        "    # load dataset\n",
        "  (X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
        "    # one hot encode target values\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_val = to_categorical(y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def balanced_acc(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "\n",
        "    tensor1 = tf.math.argmax(y_true, axis=1)\n",
        "    tensor2 = tf.math.argmax(y_pred, axis=1)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(tensor1, tensor2)\n",
        "    \n",
        "    diag = tf.linalg.tensor_diag_part (cm)\n",
        "    tpfn = tf.cast(K.sum(cm, axis = 1), tf.float32) + K.epsilon()\n",
        "    recall = tf.divide(tf.cast(diag, tf.float32),tpfn)\n",
        "    balanced_acc = K.mean(recall)\n",
        "    return balanced_acc\n",
        "\n",
        "def define_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    # compile model\n",
        "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def define_base_model(arch = the_arch, start_trainable_layer = 9999, attention=False):\n",
        "  #x = UpSampling2D(size=(7,7))(input_tensor)\n",
        "  #x = data_augmentation(input_tensor)\n",
        "  #x = layers.Rescaling(1.0 / 255)(input_tensor)  # Rescale inputs\n",
        "  if arch != 'dense':\n",
        "    input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "    if arch == 'resnet50':\n",
        "      base_model = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'inception_v3':\n",
        "      base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'ResNet':\n",
        "      base_model = ResNet(classes ,image_shape)(input_tensor)\n",
        "    x = base_model.output\n",
        "    if attention:\n",
        "      x = Attention(1024,1024,7,8)(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "    if start_trainable_layer != 9999:\n",
        "      for layer in base_model.layers[start_trainable_layer:]:\n",
        "        layer.trainable = True\n",
        "  else:\n",
        "    input_tensor = Input(shape=(2048))\n",
        "    x = input_tensor\n",
        "  #x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  #x = Dropout(0.2)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  predictions = Dense(num_classes, activation='softmax')(x)\n",
        "  model = Model(inputs=input_tensor, outputs=predictions)\n",
        "  model.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "  return model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "    # plot loss\n",
        "    plt.subplot(211)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    # plot accuracy\n",
        "    plt.subplot(212)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        " \n",
        "# scale pixels\n",
        "def norm_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm\n",
        "\n",
        "def load_isic2018_dataset(train_under_frac = 0):\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "  df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "\n",
        "  #decode one hot label\n",
        "  df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "  df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "  #random undersampling for training dataset\n",
        "  if train_under_frac !=0:\n",
        "    df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=train_under_frac).index)\n",
        "\n",
        "  #drop one-hot column\n",
        "  df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "  df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "  #make filepaths of the image\n",
        "  dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "  dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "  df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "  df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'\n",
        "  \n",
        "  #load image pixels to dataframe\n",
        "  df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "  df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, df_train, df_val\n",
        "\n",
        "def reset_dataset(df_train, df_val):\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  X_train = preprocess_image_input(X_train, the_arch)\n",
        "  X_val = preprocess_image_input(X_val, the_arch)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def SMOTE_Data(X, y, one_hot = False, k = 5, width = IMAGE_W, height = IMAGE_H, c = 3, type = 'smote'):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  if type == 'borderline':\n",
        "    sm = BorderlineSMOTE()\n",
        "  elif type == 'svm':\n",
        "    sm = SVMSMOTE()\n",
        "  elif type == 'adasyn':\n",
        "    sm = ADASYN(random_state=42, n_neighbors=k)\n",
        "  elif type == 'kmeans':\n",
        "    sm = KMeansSMOTE(k_neighbors=k, kmeans_estimator=10)\n",
        "  else:\n",
        "    sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, width * height * c)), y)\n",
        "  X_resampled = X_resampled.reshape(-1, width, height, c)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled\n",
        "\n",
        "def SMOTE_Data2(X, y, one_hot = False, k = 5):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswA0co2y1wl"
      },
      "source": [
        "#Exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnqJYIONy34l"
      },
      "outputs": [],
      "source": [
        "input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "base_model = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model(input_tensor, training=False)\n",
        "x = Attention(2048,2048,7,8)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "res50 = Model(inputs=input_tensor, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcn8hQg3J8yP"
      },
      "outputs": [],
      "source": [
        "#Train i-last layer\n",
        "# summarize feature map shapes\n",
        "for i in range(len(model.layers)):\n",
        "    layer = model.layers[i]\n",
        "    # summarize output shape\n",
        "    print(i, layer.name, layer.output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA7Af2Y73FUv"
      },
      "outputs": [],
      "source": [
        "X_train = res50.predict(X_train)\n",
        "X_val = res50.predict(X_val)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krJiAb1m3QNf"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = SMOTE_Data2(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qge6cnxQPnH6",
        "outputId": "65e4895a-7829-4ad5-8c1c-3472f03cff3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5321, 224, 224, 3)\n",
            "(5321, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 1113, 2: 1099, 1: 514, 0: 327, 6: 142, 3: 115})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "df1 = pd.read_pickle(path+\"isic2018_train.pkl\")\n",
        "X_train = df1.loc[:, df1.columns != 'y_train'].to_numpy()\n",
        "X_train = X_train.reshape(-1,224,224,3)\n",
        "y_train = df1.loc[:, df1.columns == 'y_train'].to_numpy()\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "df1 = pd.read_pickle(path+\"isic2018_val.pkl\")\n",
        "X_val = df1.loc[:, df1.columns != 'y_val'].to_numpy()\n",
        "X_val = X_val.reshape(-1,224,224,3)\n",
        "y_val = df1.loc[:, df1.columns == 'y_val'].to_numpy()\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArGWuciBt_-",
        "outputId": "54977e59-78ed-4576-a8fb-0ac292fd35e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14195, 224, 224, 3)\n",
            "(14195, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({1: 2093, 4: 2052, 6: 2042, 3: 2033, 0: 2020, 5: 2011, 2: 1944})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train, True, type='adasyn')\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7Z_nccu6QjB"
      },
      "outputs": [],
      "source": [
        "#path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "#df1 = pd.DataFrame(X_train.reshape(X_train.shape[0],-1))\n",
        "#df1['y_train'] = np.argmax(y_train, axis=1).tolist()\n",
        "#df2 = pd.DataFrame(X_val.reshape(X_val.shape[0],-1))\n",
        "#df2['y_val'] = np.argmax(y_val, axis=1).tolist()\n",
        "#df1.to_pickle(path+\"isic2018_train.pkl\")\n",
        "#df2.to_pickle(path+\"isic2018_val.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMBgWqIsAAB"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vIygrW81Ln4z",
        "outputId": "d79f4293-1084-47b2-c956-4f01ec3e7e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 1.5344 - accuracy: 0.4435 - balanced_acc: 0.4412\n",
            "Epoch 1: val_balanced_acc improved from -inf to 0.29658, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "221/221 [==============================] - 60s 210ms/step - loss: 1.5344 - accuracy: 0.4435 - balanced_acc: 0.4412 - val_loss: 1.2955 - val_accuracy: 0.5492 - val_balanced_acc: 0.2966 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 1.1522 - accuracy: 0.5939 - balanced_acc: 0.5935\n",
            "Epoch 2: val_balanced_acc improved from 0.29658 to 0.32652, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "221/221 [==============================] - 45s 200ms/step - loss: 1.1522 - accuracy: 0.5939 - balanced_acc: 0.5935 - val_loss: 1.0967 - val_accuracy: 0.6062 - val_balanced_acc: 0.3265 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 1.0207 - accuracy: 0.6351 - balanced_acc: 0.6363\n",
            "Epoch 3: val_balanced_acc improved from 0.32652 to 0.37175, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "221/221 [==============================] - 45s 204ms/step - loss: 1.0207 - accuracy: 0.6351 - balanced_acc: 0.6363 - val_loss: 1.0040 - val_accuracy: 0.6477 - val_balanced_acc: 0.3717 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.9367 - accuracy: 0.6669 - balanced_acc: 0.6664\n",
            "Epoch 4: val_balanced_acc did not improve from 0.37175\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.9367 - accuracy: 0.6669 - balanced_acc: 0.6664 - val_loss: 1.0510 - val_accuracy: 0.6373 - val_balanced_acc: 0.3615 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.8805 - accuracy: 0.6903 - balanced_acc: 0.6888\n",
            "Epoch 5: val_balanced_acc improved from 0.37175 to 0.51429, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "221/221 [==============================] - 45s 203ms/step - loss: 0.8805 - accuracy: 0.6903 - balanced_acc: 0.6888 - val_loss: 0.9409 - val_accuracy: 0.6891 - val_balanced_acc: 0.5143 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.8370 - accuracy: 0.7043 - balanced_acc: 0.7041\n",
            "Epoch 6: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 44s 198ms/step - loss: 0.8370 - accuracy: 0.7043 - balanced_acc: 0.7041 - val_loss: 0.9400 - val_accuracy: 0.6684 - val_balanced_acc: 0.3664 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.8026 - accuracy: 0.7205 - balanced_acc: 0.7207\n",
            "Epoch 7: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.8026 - accuracy: 0.7205 - balanced_acc: 0.7207 - val_loss: 0.8894 - val_accuracy: 0.6995 - val_balanced_acc: 0.4084 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.7710 - accuracy: 0.7309 - balanced_acc: 0.7287\n",
            "Epoch 8: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.7710 - accuracy: 0.7309 - balanced_acc: 0.7287 - val_loss: 0.9059 - val_accuracy: 0.6943 - val_balanced_acc: 0.4012 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.7455 - accuracy: 0.7399 - balanced_acc: 0.7398\n",
            "Epoch 9: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 195ms/step - loss: 0.7455 - accuracy: 0.7399 - balanced_acc: 0.7398 - val_loss: 0.8531 - val_accuracy: 0.7047 - val_balanced_acc: 0.4281 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.7237 - accuracy: 0.7493 - balanced_acc: 0.7481\n",
            "Epoch 10: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 195ms/step - loss: 0.7237 - accuracy: 0.7493 - balanced_acc: 0.7481 - val_loss: 0.8161 - val_accuracy: 0.7254 - val_balanced_acc: 0.4014 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.7002 - accuracy: 0.7583 - balanced_acc: 0.7584\n",
            "Epoch 11: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.7002 - accuracy: 0.7583 - balanced_acc: 0.7584 - val_loss: 0.8486 - val_accuracy: 0.7047 - val_balanced_acc: 0.4032 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.7609 - balanced_acc: 0.7617\n",
            "Epoch 12: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.6878 - accuracy: 0.7609 - balanced_acc: 0.7617 - val_loss: 0.8227 - val_accuracy: 0.7098 - val_balanced_acc: 0.4304 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.7718 - balanced_acc: 0.7689\n",
            "Epoch 13: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.6630 - accuracy: 0.7718 - balanced_acc: 0.7689 - val_loss: 0.8471 - val_accuracy: 0.6995 - val_balanced_acc: 0.4033 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.6519 - accuracy: 0.7722 - balanced_acc: 0.7723\n",
            "Epoch 14: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.6519 - accuracy: 0.7722 - balanced_acc: 0.7723 - val_loss: 0.8129 - val_accuracy: 0.6995 - val_balanced_acc: 0.3951 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.6349 - accuracy: 0.7794 - balanced_acc: 0.7784\n",
            "Epoch 15: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 195ms/step - loss: 0.6349 - accuracy: 0.7794 - balanced_acc: 0.7784 - val_loss: 0.8597 - val_accuracy: 0.6788 - val_balanced_acc: 0.4110 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.6235 - accuracy: 0.7824 - balanced_acc: 0.7818\n",
            "Epoch 16: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.6235 - accuracy: 0.7824 - balanced_acc: 0.7818 - val_loss: 0.7683 - val_accuracy: 0.7150 - val_balanced_acc: 0.4158 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.6104 - accuracy: 0.7894 - balanced_acc: 0.7908\n",
            "Epoch 17: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.6104 - accuracy: 0.7894 - balanced_acc: 0.7908 - val_loss: 0.8297 - val_accuracy: 0.6995 - val_balanced_acc: 0.4363 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.5962 - accuracy: 0.7951 - balanced_acc: 0.7942\n",
            "Epoch 18: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.5962 - accuracy: 0.7951 - balanced_acc: 0.7942 - val_loss: 0.8113 - val_accuracy: 0.6995 - val_balanced_acc: 0.3909 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.5860 - accuracy: 0.7972 - balanced_acc: 0.7986\n",
            "Epoch 19: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 196ms/step - loss: 0.5860 - accuracy: 0.7972 - balanced_acc: 0.7986 - val_loss: 0.8272 - val_accuracy: 0.6839 - val_balanced_acc: 0.3926 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.5727 - accuracy: 0.8055 - balanced_acc: 0.8028\n",
            "Epoch 20: val_balanced_acc did not improve from 0.51429\n",
            "221/221 [==============================] - 43s 195ms/step - loss: 0.5727 - accuracy: 0.8055 - balanced_acc: 0.8028 - val_loss: 0.7545 - val_accuracy: 0.7254 - val_balanced_acc: 0.4211 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# define model\n",
        "model = define_base_model('resnet50')\n",
        "#model.summary()\n",
        "hst = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n",
        "# learning curves\n",
        "summarize_diagnostics(hst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vXnW3lmCgln3",
        "outputId": "4ecb5fc8-c692-452b-e528-74e389101dc0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwwLiXUSG0IZ"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "#hst = model.fit(train_data_batches,\n",
        "#                    epochs = EPOCHS, validation_data = valid_data_batches,      \n",
        "                    #steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "#                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgjmi-4UIT-"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SPz8NH1Oylv9"
      },
      "outputs": [],
      "source": [
        "#save last model\n",
        "model.save(last_model_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS3ewyxO_anU",
        "outputId": "b44658fe-043a-4aff-d01a-1c6778104116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.8034519196900317\n",
            "balanced accuracy on training 0.8018365559948577\n",
            "accuracy on validation 0.7253886010362695\n",
            "balanced accuracy on validation 0.6494562348220885\n",
            "Score on val data:  (0.5369580910204313, 0.6494562348220885, 0.5435074254690627, None)\n"
          ]
        }
      ],
      "source": [
        "last_model = load_model(last_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = last_model.predict(X_train)\n",
        "y_val_pred = last_model.predict(X_val)\n",
        "\n",
        "#print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3IyWjdGG4Xq",
        "outputId": "48cdc8dc-b89f-4559-9537-fdafd7b7308b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.7040507220852413\n",
            "balanced accuracy on training 0.7024445035335715\n",
            "accuracy on validation 0.689119170984456\n",
            "balanced accuracy on validation 0.6634640330633362\n",
            "Score on val data:  (0.47489648033126297, 0.6634640330633362, 0.512801054310857, None)\n"
          ]
        }
      ],
      "source": [
        "best_model = load_model(best_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRWiTnO0MGh"
      },
      "source": [
        "#Cut-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGnCoIdLyDHS"
      },
      "outputs": [],
      "source": [
        "df_val_pred = pd.DataFrame(y_val_pred, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdyCbloQyWTC"
      },
      "outputs": [],
      "source": [
        "numbers = [float(x)/40 for x in range(11)]\n",
        "for i in numbers:\n",
        "    df_val_pred[i]= df_val_pred.MEL.map(lambda x: 1 if x > i else 0)\n",
        "df_val_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4SQsRx73kgk"
      },
      "outputs": [],
      "source": [
        "y_val_true= [1 if x == 4 else 0 for x in np.argmax(y_val, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcUISWFi0J05"
      },
      "outputs": [],
      "source": [
        "#num = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
        "cutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n",
        "for i in numbers:\n",
        "    cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "    total1=sum(sum(cm1))\n",
        "    Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31LSzov1tCt"
      },
      "outputs": [],
      "source": [
        "cutoff_df[['Accuracy','Sensitivity','Specificity']].plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CIKT94Jqye"
      },
      "outputs": [],
      "source": [
        "i = 0.025\n",
        "cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "total1=sum(sum(cm1))\n",
        "Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U2tkFebL_VC"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ', Accuracy)\n",
        "print('Sensitivity: ', Sensitivity)\n",
        "print('Specificity: ', Specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkPOFLehOmFg"
      },
      "outputs": [],
      "source": [
        "#change melanoma flag back to 4\n",
        "df_val_pred[df_val_pred[i] == 1] = 4\n",
        "#decode one-hot y_val_pred while use cut-off melanoma data\n",
        "condition = df_val_pred[i] == 4\n",
        "y_val_pred2 = np.where(condition, df_val_pred[i], np.argmax(y_val_pred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOVl6dWlTDLo"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ',accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))\n",
        "print('Balanced accuracy: ',balanced_accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvYutTKRhR_"
      },
      "outputs": [],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_val, axis=1), y_val_pred2)\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtvW3YeaLlC"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-1yjWGeKs7"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "#labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "#f = plt.figure(figsize=(15, 6))\n",
        "#s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "#s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic95mefkpG3"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "df_test['image_px'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60LYAT7VsNOZ"
      },
      "outputs": [],
      "source": [
        "X_test = np.asarray(df_test['image_px'].tolist())\n",
        "print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXnnIIwC4cHE"
      },
      "outputs": [],
      "source": [
        "#preprocess\n",
        "X_test = preprocess_image_input(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeDTXdaMLmyU"
      },
      "outputs": [],
      "source": [
        "#X_test = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIX0AmEFNv3Y"
      },
      "outputs": [],
      "source": [
        "# predicting\n",
        "#CHANGE THE MODEL IF NECESSARY\n",
        "#X_test2 = model1.predict(X_test)\n",
        "Y_pred2 = model.predict(X_test2)\n",
        "print(\"Y_pred2\", Y_pred2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]\n",
        "df_pred.set_index(\"image\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ynyd8PjT589"
      },
      "outputs": [],
      "source": [
        "#update MEL data using cut-off value\n",
        "df_pred.MEL[df_pred.MEL > i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjRdONoQVMq0"
      },
      "outputs": [],
      "source": [
        "df_pred.loc[df_pred.MEL > i, ['NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_SMOTE_Attention.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0MghVs0tsGw"
      },
      "source": [
        "result: 0.656"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE8Ziq-BlEP4"
      },
      "source": [
        "#Oversampling on feature map level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtgmvyhCndpB"
      },
      "outputs": [],
      "source": [
        "i = 176"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm05Zet_B5am"
      },
      "outputs": [],
      "source": [
        "for i in range(len(model.layers)):\n",
        "  layer = model.layers[i]\n",
        "  print(i, layer.name, layer.output_shape, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqeSic6NmLsR"
      },
      "outputs": [],
      "source": [
        "# redefine model to output right after the first hidden layer\n",
        "model1 = Model(inputs=model.inputs, outputs=model.layers[i].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVHYG9Rwm28i"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "X_train_fm = model1.predict(X_train)\n",
        "X_val_fm = model1.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNozN8-wDUNL"
      },
      "outputs": [],
      "source": [
        "X_train_fm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19hK7aQNeAQo"
      },
      "outputs": [],
      "source": [
        "X_train_fm_ov, y_train_ov = SMOTE_Data2(X_train_fm, y_train, True, 5)\n",
        "print(X_train_fm_ov.shape)\n",
        "print(y_train_ov.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train_ov, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qP4iyYcnAYa"
      },
      "outputs": [],
      "source": [
        "model2 = Model(inputs=model.layers[i].output, outputs=model.layers[len(model.layers)-1].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzdjs0WbvDB0"
      },
      "outputs": [],
      "source": [
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/last_model_no.h5'\n",
        "model2.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst = model2.fit(X_train_fm_ov, y_train_ov, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val_fm, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train_fm_ov.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XhlbWn--8Or"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW-_U6vFpIci"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "y_train_pred = model2.predict(X_train_fm_ov)\n",
        "y_val_pred = model2.predict(X_val_fm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLop0YK-ZK40"
      },
      "outputs": [],
      "source": [
        "print('accuracy on training',accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load ISIC 2018 Challange Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3P7IjyLuZGY"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_isic2018_dataset(train_under_frac = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IncA-_o_n5w"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = y_train.value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(y_train == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "labels = sorted(df1['y_train'].unique())\n",
        "for label in range(7):\n",
        "    plot_images_per_label(df1, label, 3, (12,9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asV1O58Lrq-R"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.fromarray(X_train[0], 'RGB')\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "#df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "#df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "#df_train = df_train.drop_duplicates(subset=['lesion_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmX_Uqbmj-tN"
      },
      "outputs": [],
      "source": [
        "from numpy import moveaxis\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_el = np.inf\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 300       # how many epochs to run for\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "args['patience'] = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NydOdPMajEfT"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        # [(W−K+2P)/S]+1\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(7, stride=7),\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),# 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False), # 8\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),# 4\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 0, bias=False),#14\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**3 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        # H_out ​= (H_in​−1)*stride[0] − 2×padding[0] + dilation[0]×(kernel_size[0]−1) + output_padding[0] + 1\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4), #10\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4), #13\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h, 4),# 16\n",
        "            nn.BatchNorm2d(self.dim_h),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h, 3, 4, 2, 1),# 32\n",
        "            nn.UpsamplingBilinear2d(scale_factor=7),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**3, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def DeepSMOTE_train(X_train, y_train, one_hot = False):\n",
        "  from torch.utils.data import TensorDataset\n",
        "  import os\n",
        "\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #X_train = X_train.astype('float32') / 255.\n",
        "  \n",
        "  batch_size = args['batch_size']\n",
        "  patience = args['patience']\n",
        "  encoder = Encoder(args)\n",
        "  decoder = Decoder(args)\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  print(device)\n",
        "  decoder = decoder.to(device)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "  #decoder loss function\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  num_workers = 0\n",
        "\n",
        "  #torch.Tensor returns float so if want long then use torch.tensor\n",
        "  tensor_x = torch.from_numpy(X_train.copy())#torch.Tensor(X_train)\n",
        "  tensor_y = torch.tensor(y_train,dtype=torch.long)\n",
        "  mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "  train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "      batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "  best_loss = np.inf\n",
        "\n",
        "  enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "  dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "  for epoch in range(args['epochs']):\n",
        "      train_loss = 0.0\n",
        "      tmse_loss = 0.0\n",
        "      tdiscr_loss = 0.0\n",
        "      # train for one epoch -- set nets to train mode\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "  \n",
        "      for images,labs in train_loader:\n",
        "      \n",
        "          # zero gradients for each batch\n",
        "          encoder.zero_grad()\n",
        "          decoder.zero_grad()\n",
        "          images, labs = images.to(device), labs.to(device)\n",
        "          labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "          # run images\n",
        "          z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "      \n",
        "          x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "          mse = criterion(x_hat,images)\n",
        "                  \n",
        "          resx = []\n",
        "          resy = []\n",
        "      \n",
        "          tc = np.random.choice(num_classes,1)\n",
        "          #tc = 9\n",
        "          xbeg = X_train[y_train == tc]\n",
        "          ybeg = y_train[y_train == tc] \n",
        "          xlen = len(xbeg)\n",
        "          nsamp = min(xlen, 100)\n",
        "          ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "          xclass = xbeg[ind]\n",
        "          yclass = ybeg[ind]\n",
        "      \n",
        "          xclen = len(xclass)\n",
        "          xcminus = np.arange(1,xclen)\n",
        "          \n",
        "          xcplus = np.append(xcminus,0)\n",
        "          xcnew = (xclass[[xcplus],:])\n",
        "          xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "      \n",
        "          xcnew = torch.Tensor(xcnew)\n",
        "          xcnew = xcnew.to(device)\n",
        "      \n",
        "          #encode xclass to feature space\n",
        "          xclass = torch.Tensor(xclass)\n",
        "          xclass = xclass.to(device)\n",
        "          xclass = encoder(xclass)\n",
        "      \n",
        "          xclass = xclass.detach().cpu().numpy()\n",
        "      \n",
        "          xc_enc = (xclass[[xcplus],:])\n",
        "          xc_enc = np.squeeze(xc_enc)\n",
        "      \n",
        "          xc_enc = torch.Tensor(xc_enc)\n",
        "          xc_enc = xc_enc.to(device)\n",
        "          \n",
        "          ximg = decoder(xc_enc)\n",
        "          \n",
        "          mse2 = criterion(ximg,xcnew)\n",
        "      \n",
        "          comb_loss = mse2 + mse\n",
        "          comb_loss.backward()\n",
        "      \n",
        "          enc_optim.step()\n",
        "          dec_optim.step()\n",
        "      \n",
        "          train_loss += comb_loss.item()*images.size(0)\n",
        "          tmse_loss += mse.item()*images.size(0)\n",
        "          tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "      train_loss = train_loss/len(train_loader)\n",
        "      tmse_loss = tmse_loss/len(train_loader)\n",
        "      tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "      print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "              train_loss,tmse_loss,tdiscr_loss))\n",
        "      \n",
        "  \n",
        "  \n",
        "      #store the best encoder and decoder models\n",
        "      #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "      #necessary for illustration purposes\n",
        "      if train_loss < best_loss:\n",
        "          print('Saving..')\n",
        "          patience = args['patience']\n",
        "          path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_enc.pth'\n",
        "          path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_dec.pth'\n",
        "        \n",
        "          torch.save(encoder.state_dict(), path_enc)\n",
        "          torch.save(decoder.state_dict(), path_dec)\n",
        "  \n",
        "          best_loss = train_loss\n",
        "      else:\n",
        "          patience = patience - 1\n",
        "\n",
        "      if patience == 0:\n",
        "          print('Out of patience. \\n')\n",
        "          break\n",
        "\n",
        "def DeepSMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  batch_size = args['batch_size']\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #Generate artificial images\n",
        "  import torch\n",
        "  np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "  #path on the computer where the models are stored\n",
        "  modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/'\n",
        "\n",
        "  path_enc = modpth + '/bst_enc.pth'\n",
        "  path_dec = modpth + '/bst_dec.pth'\n",
        "  \n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  encoder = Encoder(args)\n",
        "  encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  decoder = Decoder(args)\n",
        "  decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "  decoder = decoder.to(device)\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  resx = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      #encode xclass to feature space\n",
        "      xclass = torch.Tensor(xclass)\n",
        "      xclass = xclass.to(device)\n",
        "      xclass = encoder(xclass)\n",
        "          \n",
        "      xclass = xclass.detach().cpu().numpy()\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n == 0:\n",
        "        continue\n",
        "#        resx2 = []\n",
        "#        resy2 = []\n",
        "#        for j in range(batch_size, n+batch_size+1, batch_size):\n",
        "#          if j <= n:\n",
        "#            batch_size_max = batch_size\n",
        "#          elif n % batch_size != 0:\n",
        "#            batch_size_max = n%batch_size\n",
        "#          else:\n",
        "#            break\n",
        "#          xsamp, ysamp = G_SM(xclass,yclass,batch_size_max,i)\n",
        "      xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "  \n",
        "      \"\"\"to generate samples for resnet\"\"\"   \n",
        "      xsamp = torch.Tensor(xsamp)\n",
        "      xsamp = xsamp.to(device)\n",
        "      ximg = decoder(xsamp)\n",
        "\n",
        "      ximn = ximg.detach().cpu().numpy()\n",
        "#        resx2.append(ximn)\n",
        "#        resy2.append(ysamp)\n",
        "#        \n",
        "#        resx2 = np.vstack(resx2)\n",
        "#        resy2 = np.hstack(resy2)\n",
        "      resx.append(ximn)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx1 = np.vstack(resx)\n",
        "  resy1 = np.hstack(resy)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = X_train.reshape(X_train.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "  X_train = moveaxis(X_train, 1, 3)\n",
        "  X_train = X_train * max_el\n",
        "  return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6qneWL_Bs2U"
      },
      "outputs": [],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr1jnSM7yzJc"
      },
      "outputs": [],
      "source": [
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1aAQBmiy0K"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['balanced_acc'])\n",
        "plt.plot(hst2.history['val_balanced_acc'])\n",
        "plt.title('model balance_acc after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UswA0co2y1wl",
        "iDRWiTnO0MGh",
        "eaK4zbtoaAaC",
        "3K908bbiYwbS",
        "kE8Ziq-BlEP4",
        "RcRGeofw-8tK",
        "cNBXx28B9yGu",
        "0jrJ33lUDkCM",
        "B2PgksTFkOAq"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}