{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Oversampling/blob/main/Skin_Cancer_Diagnosis_using_No-Oversampling%20(benchmark).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "16de3d33-930b-4dc7-c4a9-f93dfd04f198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfcFpsBwM0d4"
      },
      "source": [
        "#Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C_s6OIGKM26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "371bd24a-4bf9-491a-e0ec-78b7eb06e6d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ff488085aaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_current_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_preprocessing_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Image preprocessing layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m \u001b[0mcython_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'core'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tk\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,AveragePooling2D,BatchNormalization,Add,ZeroPadding2D,Flatten,Dense,Input,LeakyReLU,Softmax,ReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class Attention(tk.layers.Layer):\n",
        "    \n",
        "    def __init__(self,input_channels,output_channel,kernel_size,groups):\n",
        "        super().__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channel = output_channel    \n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = 1\n",
        "        self.groups = groups\n",
        "\n",
        "        assert output_channel % groups == 0\n",
        "        \n",
        "        self.rel_h = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,kernel_size,1,output_channel//2),stddev = 0.1)) \n",
        "        #output_channels//2 is the number of channels on which the relative position will be considered,1 denotes the number of those filters and the one after that too and (kernel_size,1) denotes the size of that filter\n",
        "        self.rel_w = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,1,kernel_size,output_channel//2),stddev = 0.1)) \n",
        "\n",
        "        self.key_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.query_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.value_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "\n",
        "    def call(self,x):\n",
        "        \n",
        "        batch,height,width,channels = x.shape\n",
        "        x_padded = ZeroPadding2D(padding=(self.kernel_size//2,self.kernel_size//2))(x)\n",
        "        query = self.query_weights(x)\n",
        "        value = self.value_weights(x_padded)\n",
        "        key = self.key_weights(x_padded)\n",
        "        #key,query and value will have the shape of (batch,height,width,depth)\n",
        "        keys = tf.image.extract_patches(images = key,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        value = tf.image.extract_patches(images = value,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        no_of_kernels = key.shape[-2] - self.kernel_size + 1\n",
        "        keys = tf.reshape(keys,shape = (-1,no_of_kernels, no_of_kernels,self.kernel_size,self.kernel_size,self.output_channel))\n",
        "        key_split_h,key_split_w = tf.split(keys,num_or_size_splits = 2,axis = -1)\n",
        "        key_with_rel = tk.layers.concatenate([key_split_h + self.rel_h,key_split_w + self.rel_w],axis = -1) \n",
        "        \n",
        "        #reshaping the query and key\n",
        "        key_with_rel = tf.reshape(key_with_rel,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        query  = tf.reshape(query,(-1,self.groups,no_of_kernels,no_of_kernels,1,self.output_channel//self.groups))        \n",
        "        value = tf.reshape(value,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        \n",
        "        #multiplication  of key and query\n",
        "        #assert key_with_rel.shape == query.shape        \n",
        "        key_prod_query = query*key_with_rel\n",
        "        \n",
        "        # Now the function is passed through the softmax and is multiplied with the values\n",
        "        s = Softmax(axis = -2)(key_prod_query)\n",
        "        y = tf.einsum('bnchwk,bnchwk->bnchk',s,value)\n",
        "        y = tf.reshape(y,(-1,height,width,self.output_channel))\n",
        "        return y\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            \"input_channels\": self.input_channels, \n",
        "            \"output_channel\": self.output_channel, \n",
        "            \"kernel_size\": self.kernel_size, \n",
        "            \"stride\": self.stride, \n",
        "            \"groups\": self.groups, \n",
        "            \"rel_h\": self.rel_h, \n",
        "            \"rel_w\": self.rel_w, \n",
        "            \"key_weights\": self.key_weights, \n",
        "            \"query_weights\": self.query_weights, \n",
        "            \"value_weights\": self.value_weights\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_x4c0_DTkaa"
      },
      "source": [
        "#Library, atribut, and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR2MJBYq-oiB",
        "outputId": "df95573f-4750-453c-c31c-d42f0e6e1d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 224\n",
        "IMAGE_H = 224\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)\n",
        "num_classes = 7\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "opt_adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "opt_SGD = SGD(learning_rate=0.001)\n",
        "the_arch = 'resnet50'\n",
        "\n",
        "#Data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2), \n",
        "  layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),\n",
        "  layers.experimental.preprocessing.RandomTranslation(0.3, 0.3, fill_mode='reflect', interpolation='bilinear',)\n",
        "])\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model_attention.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/last_model_attention.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_balanced_acc', patience=20, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_balanced_acc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JffFid9sOXeo"
      },
      "outputs": [],
      "source": [
        "# load train and test dataset\n",
        "def preprocess_image_input(input_images, arch = the_arch):\n",
        "  input_images = input_images.astype('float32')\n",
        "  if arch == 'inception_v3':\n",
        "    output_ims = tf.keras.applications.inception_v3.preprocess_input(input_images)\n",
        "  else:\n",
        "    output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def load_cifar10_dataset():\n",
        "  from keras.datasets import cifar10\n",
        "    # load dataset\n",
        "  (X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
        "    # one hot encode target values\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_val = to_categorical(y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def balanced_acc(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "\n",
        "    tensor1 = tf.math.argmax(y_true, axis=1)\n",
        "    tensor2 = tf.math.argmax(y_pred, axis=1)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(tensor1, tensor2)\n",
        "    \n",
        "    diag = tf.linalg.tensor_diag_part (cm)\n",
        "    tpfn = tf.cast(K.sum(cm, axis = 1), tf.float32) + K.epsilon()\n",
        "    recall = tf.divide(tf.cast(diag, tf.float32),tpfn)\n",
        "    balanced_acc = K.mean(recall)\n",
        "    return balanced_acc\n",
        "\n",
        "def define_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    # compile model\n",
        "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def define_base_model(arch = the_arch, start_trainable_layer = 9999, attention=False):\n",
        "  #x = UpSampling2D(size=(7,7))(input_tensor)\n",
        "  #x = data_augmentation(input_tensor)\n",
        "  #x = layers.Rescaling(1.0 / 255)(input_tensor)  # Rescale inputs\n",
        "  if arch != 'dense':\n",
        "    input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "    if arch == 'resnet50':\n",
        "      base_model = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'inception_v3':\n",
        "      base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'ResNet':\n",
        "      base_model = ResNet(classes ,image_shape)(input_tensor)\n",
        "    x = base_model.output\n",
        "    if attention:\n",
        "      x = Attention(1024,1024,7,8)(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "    if start_trainable_layer != 9999:\n",
        "      for layer in base_model.layers[start_trainable_layer:]:\n",
        "        layer.trainable = True\n",
        "  else:\n",
        "    input_tensor = Input(shape=(2048))\n",
        "    x = input_tensor\n",
        "  #x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  #x = Dropout(0.2)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  predictions = Dense(num_classes, activation='softmax')(x)\n",
        "  model = Model(inputs=input_tensor, outputs=predictions)\n",
        "  model.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "  return model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "    # plot loss\n",
        "    plt.subplot(211)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    # plot accuracy\n",
        "    plt.subplot(212)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        " \n",
        "# scale pixels\n",
        "def norm_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm\n",
        "\n",
        "def load_isic2018_dataset(train_under_frac = 0):\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "  df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "\n",
        "  #decode one hot label\n",
        "  df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "  df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "  #random undersampling for training dataset\n",
        "  if train_under_frac !=0:\n",
        "    df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=train_under_frac).index)\n",
        "\n",
        "  #drop one-hot column\n",
        "  df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "  df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "  #make filepaths of the image\n",
        "  dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "  dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "  df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "  df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'\n",
        "  \n",
        "  #load image pixels to dataframe\n",
        "  df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "  df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, df_train, df_val\n",
        "\n",
        "def reset_dataset(df_train, df_val):\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  X_train = preprocess_image_input(X_train, the_arch)\n",
        "  X_val = preprocess_image_input(X_val, the_arch)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def SMOTE_Data(X, y, one_hot = False, k = 5, width = IMAGE_W, height = IMAGE_H, c = 3, type = 'smote'):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  if type == 'borderline':\n",
        "    sm = BorderlineSMOTE(random_state=42, k_neighbors=k)\n",
        "  elif type == 'svm':\n",
        "    sm = SVMSMOTE()\n",
        "  elif type == 'adasyn':\n",
        "    sm = ADASYN(random_state=42, n_neighbors=k)\n",
        "  elif type == 'kmeans':\n",
        "    sm = KMeansSMOTE(k_neighbors=k, kmeans_estimator=10)\n",
        "  else:\n",
        "    sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, width * height * c)), y)\n",
        "  X_resampled = X_resampled.reshape(-1, width, height, c)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled\n",
        "\n",
        "def SMOTE_Data2(X, y, one_hot = False, k = 5):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswA0co2y1wl"
      },
      "source": [
        "#Exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnqJYIONy34l"
      },
      "outputs": [],
      "source": [
        "input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "base_model = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model(input_tensor, training=False)\n",
        "x = Attention(2048,2048,7,8)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "res50 = Model(inputs=input_tensor, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcn8hQg3J8yP"
      },
      "outputs": [],
      "source": [
        "#Train i-last layer\n",
        "# summarize feature map shapes\n",
        "for i in range(len(model.layers)):\n",
        "    layer = model.layers[i]\n",
        "    # summarize output shape\n",
        "    print(i, layer.name, layer.output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA7Af2Y73FUv"
      },
      "outputs": [],
      "source": [
        "X_train = res50.predict(X_train)\n",
        "X_val = res50.predict(X_val)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krJiAb1m3QNf"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = SMOTE_Data2(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qge6cnxQPnH6",
        "outputId": "d97ad266-fc52-4943-a893-45ee96c575bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5321, 224, 224, 3)\n",
            "(5321, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 1113, 2: 1099, 1: 514, 0: 327, 6: 142, 3: 115})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "df1 = pd.read_pickle(path+\"isic2018_train.pkl\")\n",
        "X_train = df1.loc[:, df1.columns != 'y_train'].to_numpy()\n",
        "X_train = X_train.reshape(-1,224,224,3)\n",
        "y_train = df1.loc[:, df1.columns == 'y_train'].to_numpy()\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "df1 = pd.read_pickle(path+\"isic2018_val.pkl\")\n",
        "X_val = df1.loc[:, df1.columns != 'y_val'].to_numpy()\n",
        "X_val = X_val.reshape(-1,224,224,3)\n",
        "y_val = df1.loc[:, df1.columns == 'y_val'].to_numpy()\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArGWuciBt_-",
        "outputId": "3047fa09-1847-4e56-9481-2c58d7fa3dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14077, 224, 224, 3)\n",
            "(14077, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 2011, 2: 2011, 3: 2011, 0: 2011, 1: 2011, 6: 2011})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train, True, type='borderline')\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7Z_nccu6QjB"
      },
      "outputs": [],
      "source": [
        "#path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "#df1 = pd.DataFrame(X_train.reshape(X_train.shape[0],-1))\n",
        "#df1['y_train'] = np.argmax(y_train, axis=1).tolist()\n",
        "#df2 = pd.DataFrame(X_val.reshape(X_val.shape[0],-1))\n",
        "#df2['y_val'] = np.argmax(y_val, axis=1).tolist()\n",
        "#df1.to_pickle(path+\"isic2018_train.pkl\")\n",
        "#df2.to_pickle(path+\"isic2018_val.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMBgWqIsAAB"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vIygrW81Ln4z",
        "outputId": "f43b01dc-b37a-451e-830b-ee0249054f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 1.5487 - accuracy: 0.4134 - balanced_acc: 0.2008\n",
            "Epoch 1: val_balanced_acc improved from -inf to 0.17962, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 27s 222ms/step - loss: 1.5487 - accuracy: 0.4134 - balanced_acc: 0.2008 - val_loss: 1.0286 - val_accuracy: 0.6373 - val_balanced_acc: 0.1796 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 1.2991 - accuracy: 0.5088 - balanced_acc: 0.2762\n",
            "Epoch 2: val_balanced_acc improved from 0.17962 to 0.23928, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 19s 231ms/step - loss: 1.2991 - accuracy: 0.5088 - balanced_acc: 0.2762 - val_loss: 0.9346 - val_accuracy: 0.6839 - val_balanced_acc: 0.2393 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 1.2016 - accuracy: 0.5459 - balanced_acc: 0.3147\n",
            "Epoch 3: val_balanced_acc improved from 0.23928 to 0.24046, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 221ms/step - loss: 1.2016 - accuracy: 0.5459 - balanced_acc: 0.3147 - val_loss: 0.9116 - val_accuracy: 0.6684 - val_balanced_acc: 0.2405 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 1.1325 - accuracy: 0.5710 - balanced_acc: 0.3486\n",
            "Epoch 4: val_balanced_acc improved from 0.24046 to 0.37545, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 217ms/step - loss: 1.1325 - accuracy: 0.5710 - balanced_acc: 0.3486 - val_loss: 0.8516 - val_accuracy: 0.7098 - val_balanced_acc: 0.3754 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.5863 - balanced_acc: 0.3609\n",
            "Epoch 5: val_balanced_acc did not improve from 0.37545\n",
            "83/83 [==============================] - 16s 197ms/step - loss: 1.0878 - accuracy: 0.5863 - balanced_acc: 0.3609 - val_loss: 0.8680 - val_accuracy: 0.6684 - val_balanced_acc: 0.3728 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 1.0490 - accuracy: 0.6040 - balanced_acc: 0.3827\n",
            "Epoch 6: val_balanced_acc did not improve from 0.37545\n",
            "83/83 [==============================] - 17s 200ms/step - loss: 1.0490 - accuracy: 0.6040 - balanced_acc: 0.3827 - val_loss: 0.8492 - val_accuracy: 0.6684 - val_balanced_acc: 0.3720 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 1.0083 - accuracy: 0.6222 - balanced_acc: 0.4065\n",
            "Epoch 7: val_balanced_acc improved from 0.37545 to 0.38147, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 219ms/step - loss: 1.0083 - accuracy: 0.6222 - balanced_acc: 0.4065 - val_loss: 0.7875 - val_accuracy: 0.6995 - val_balanced_acc: 0.3815 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.9932 - accuracy: 0.6220 - balanced_acc: 0.4199\n",
            "Epoch 8: val_balanced_acc improved from 0.38147 to 0.38925, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 19s 224ms/step - loss: 0.9932 - accuracy: 0.6220 - balanced_acc: 0.4199 - val_loss: 0.7783 - val_accuracy: 0.7047 - val_balanced_acc: 0.3892 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.9667 - accuracy: 0.6365 - balanced_acc: 0.4408\n",
            "Epoch 9: val_balanced_acc did not improve from 0.38925\n",
            "83/83 [==============================] - 16s 198ms/step - loss: 0.9667 - accuracy: 0.6365 - balanced_acc: 0.4408 - val_loss: 0.7435 - val_accuracy: 0.7306 - val_balanced_acc: 0.3879 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.9509 - accuracy: 0.6447 - balanced_acc: 0.4532\n",
            "Epoch 10: val_balanced_acc improved from 0.38925 to 0.42767, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 217ms/step - loss: 0.9509 - accuracy: 0.6447 - balanced_acc: 0.4532 - val_loss: 0.7390 - val_accuracy: 0.7254 - val_balanced_acc: 0.4277 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.9191 - accuracy: 0.6586 - balanced_acc: 0.4623\n",
            "Epoch 11: val_balanced_acc improved from 0.42767 to 0.44789, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 217ms/step - loss: 0.9191 - accuracy: 0.6586 - balanced_acc: 0.4623 - val_loss: 0.7562 - val_accuracy: 0.7254 - val_balanced_acc: 0.4479 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.6561 - balanced_acc: 0.4801\n",
            "Epoch 12: val_balanced_acc improved from 0.44789 to 0.45196, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 219ms/step - loss: 0.9134 - accuracy: 0.6561 - balanced_acc: 0.4801 - val_loss: 0.7430 - val_accuracy: 0.7358 - val_balanced_acc: 0.4520 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.8927 - accuracy: 0.6635 - balanced_acc: 0.4752\n",
            "Epoch 13: val_balanced_acc improved from 0.45196 to 0.48424, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 218ms/step - loss: 0.8927 - accuracy: 0.6635 - balanced_acc: 0.4752 - val_loss: 0.7339 - val_accuracy: 0.7254 - val_balanced_acc: 0.4842 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.8803 - accuracy: 0.6654 - balanced_acc: 0.4860\n",
            "Epoch 14: val_balanced_acc did not improve from 0.48424\n",
            "83/83 [==============================] - 17s 200ms/step - loss: 0.8803 - accuracy: 0.6654 - balanced_acc: 0.4860 - val_loss: 0.7206 - val_accuracy: 0.7409 - val_balanced_acc: 0.3636 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.8750 - accuracy: 0.6732 - balanced_acc: 0.4958\n",
            "Epoch 15: val_balanced_acc improved from 0.48424 to 0.51331, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 215ms/step - loss: 0.8750 - accuracy: 0.6732 - balanced_acc: 0.4958 - val_loss: 0.7231 - val_accuracy: 0.7565 - val_balanced_acc: 0.5133 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.8484 - accuracy: 0.6812 - balanced_acc: 0.5001\n",
            "Epoch 16: val_balanced_acc did not improve from 0.51331\n",
            "83/83 [==============================] - 17s 199ms/step - loss: 0.8484 - accuracy: 0.6812 - balanced_acc: 0.5001 - val_loss: 0.7043 - val_accuracy: 0.7409 - val_balanced_acc: 0.3704 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.8474 - accuracy: 0.6797 - balanced_acc: 0.5223\n",
            "Epoch 17: val_balanced_acc did not improve from 0.51331\n",
            "83/83 [==============================] - 17s 200ms/step - loss: 0.8474 - accuracy: 0.6797 - balanced_acc: 0.5223 - val_loss: 0.7040 - val_accuracy: 0.7461 - val_balanced_acc: 0.3836 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.8239 - accuracy: 0.6932 - balanced_acc: 0.5258\n",
            "Epoch 18: val_balanced_acc improved from 0.51331 to 0.53664, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 216ms/step - loss: 0.8239 - accuracy: 0.6932 - balanced_acc: 0.5258 - val_loss: 0.6918 - val_accuracy: 0.7617 - val_balanced_acc: 0.5366 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.8319 - accuracy: 0.6884 - balanced_acc: 0.5190\n",
            "Epoch 19: val_balanced_acc improved from 0.53664 to 0.53712, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "83/83 [==============================] - 18s 216ms/step - loss: 0.8319 - accuracy: 0.6884 - balanced_acc: 0.5190 - val_loss: 0.6866 - val_accuracy: 0.7668 - val_balanced_acc: 0.5371 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.8101 - accuracy: 0.6962 - balanced_acc: 0.5368\n",
            "Epoch 20: val_balanced_acc did not improve from 0.53712\n",
            "83/83 [==============================] - 17s 199ms/step - loss: 0.8101 - accuracy: 0.6962 - balanced_acc: 0.5368 - val_loss: 0.6782 - val_accuracy: 0.7617 - val_balanced_acc: 0.4058 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# define model\n",
        "model = define_base_model('resnet50')\n",
        "#model.summary()\n",
        "hst = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n",
        "# learning curves\n",
        "summarize_diagnostics(hst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vXnW3lmCgln3",
        "outputId": "95e8a352-830d-4dc1-c319-dfd8dde69651"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwwLiXUSG0IZ"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "#hst = model.fit(train_data_batches,\n",
        "#                    epochs = EPOCHS, validation_data = valid_data_batches,      \n",
        "                    #steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "#                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgjmi-4UIT-"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SPz8NH1Oylv9"
      },
      "outputs": [],
      "source": [
        "#save last model\n",
        "model.save(last_model_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS3ewyxO_anU",
        "outputId": "d378d2fb-8e9c-43c2-bd08-a6ef2ec2b6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.7047547453486187\n",
            "balanced accuracy on training 0.5570036738034814\n",
            "accuracy on validation 0.7616580310880829\n",
            "balanced accuracy on validation 0.6126031343801378\n",
            "Score on val data:  (0.6907316772862991, 0.6126031343801378, 0.6247121925693354, None)\n"
          ]
        }
      ],
      "source": [
        "last_model = load_model(last_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = last_model.predict(X_train)\n",
        "y_val_pred = last_model.predict(X_val)\n",
        "\n",
        "#print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3IyWjdGG4Xq",
        "outputId": "724c4193-6232-4690-9202-3d5c202b420a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.7032512685585416\n",
            "balanced accuracy on training 0.5493507956227827\n",
            "accuracy on validation 0.7668393782383419\n",
            "balanced accuracy on validation 0.6332104016773006\n",
            "Score on val data:  (0.7368454633398137, 0.6332104016773006, 0.6414340632387526, None)\n"
          ]
        }
      ],
      "source": [
        "best_model = load_model(best_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRWiTnO0MGh"
      },
      "source": [
        "#Cut-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGnCoIdLyDHS"
      },
      "outputs": [],
      "source": [
        "df_val_pred = pd.DataFrame(y_val_pred, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdyCbloQyWTC"
      },
      "outputs": [],
      "source": [
        "numbers = [float(x)/40 for x in range(11)]\n",
        "for i in numbers:\n",
        "    df_val_pred[i]= df_val_pred.MEL.map(lambda x: 1 if x > i else 0)\n",
        "df_val_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4SQsRx73kgk"
      },
      "outputs": [],
      "source": [
        "y_val_true= [1 if x == 4 else 0 for x in np.argmax(y_val, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcUISWFi0J05"
      },
      "outputs": [],
      "source": [
        "#num = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
        "cutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n",
        "for i in numbers:\n",
        "    cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "    total1=sum(sum(cm1))\n",
        "    Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31LSzov1tCt"
      },
      "outputs": [],
      "source": [
        "cutoff_df[['Accuracy','Sensitivity','Specificity']].plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CIKT94Jqye"
      },
      "outputs": [],
      "source": [
        "i = 0.025\n",
        "cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "total1=sum(sum(cm1))\n",
        "Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U2tkFebL_VC"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ', Accuracy)\n",
        "print('Sensitivity: ', Sensitivity)\n",
        "print('Specificity: ', Specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkPOFLehOmFg"
      },
      "outputs": [],
      "source": [
        "#change melanoma flag back to 4\n",
        "df_val_pred[df_val_pred[i] == 1] = 4\n",
        "#decode one-hot y_val_pred while use cut-off melanoma data\n",
        "condition = df_val_pred[i] == 4\n",
        "y_val_pred2 = np.where(condition, df_val_pred[i], np.argmax(y_val_pred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOVl6dWlTDLo"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ',accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))\n",
        "print('Balanced accuracy: ',balanced_accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvYutTKRhR_"
      },
      "outputs": [],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_val, axis=1), y_val_pred2)\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtvW3YeaLlC"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-1yjWGeKs7"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "#labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "#f = plt.figure(figsize=(15, 6))\n",
        "#s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "#s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic95mefkpG3"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "df_test['image_px'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60LYAT7VsNOZ"
      },
      "outputs": [],
      "source": [
        "X_test = np.asarray(df_test['image_px'].tolist())\n",
        "print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXnnIIwC4cHE"
      },
      "outputs": [],
      "source": [
        "#preprocess\n",
        "X_test = preprocess_image_input(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeDTXdaMLmyU"
      },
      "outputs": [],
      "source": [
        "#X_test = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIX0AmEFNv3Y"
      },
      "outputs": [],
      "source": [
        "# predicting\n",
        "#CHANGE THE MODEL IF NECESSARY\n",
        "#X_test2 = model1.predict(X_test)\n",
        "Y_pred2 = model.predict(X_test2)\n",
        "print(\"Y_pred2\", Y_pred2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]\n",
        "df_pred.set_index(\"image\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ynyd8PjT589"
      },
      "outputs": [],
      "source": [
        "#update MEL data using cut-off value\n",
        "df_pred.MEL[df_pred.MEL > i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjRdONoQVMq0"
      },
      "outputs": [],
      "source": [
        "df_pred.loc[df_pred.MEL > i, ['NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_SMOTE_Attention.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0MghVs0tsGw"
      },
      "source": [
        "result: 0.656"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE8Ziq-BlEP4"
      },
      "source": [
        "#Oversampling on feature map level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtgmvyhCndpB"
      },
      "outputs": [],
      "source": [
        "i = 176"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm05Zet_B5am"
      },
      "outputs": [],
      "source": [
        "for i in range(len(model.layers)):\n",
        "  layer = model.layers[i]\n",
        "  print(i, layer.name, layer.output_shape, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqeSic6NmLsR"
      },
      "outputs": [],
      "source": [
        "# redefine model to output right after the first hidden layer\n",
        "model1 = Model(inputs=model.inputs, outputs=model.layers[i].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVHYG9Rwm28i"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "X_train_fm = model1.predict(X_train)\n",
        "X_val_fm = model1.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNozN8-wDUNL"
      },
      "outputs": [],
      "source": [
        "X_train_fm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19hK7aQNeAQo"
      },
      "outputs": [],
      "source": [
        "X_train_fm_ov, y_train_ov = SMOTE_Data2(X_train_fm, y_train, True, 5)\n",
        "print(X_train_fm_ov.shape)\n",
        "print(y_train_ov.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train_ov, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qP4iyYcnAYa"
      },
      "outputs": [],
      "source": [
        "model2 = Model(inputs=model.layers[i].output, outputs=model.layers[len(model.layers)-1].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzdjs0WbvDB0"
      },
      "outputs": [],
      "source": [
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/last_model_no.h5'\n",
        "model2.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst = model2.fit(X_train_fm_ov, y_train_ov, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val_fm, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train_fm_ov.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XhlbWn--8Or"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW-_U6vFpIci"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "y_train_pred = model2.predict(X_train_fm_ov)\n",
        "y_val_pred = model2.predict(X_val_fm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLop0YK-ZK40"
      },
      "outputs": [],
      "source": [
        "print('accuracy on training',accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load ISIC 2018 Challange Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3P7IjyLuZGY"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_isic2018_dataset(train_under_frac = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IncA-_o_n5w"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = y_train.value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(y_train == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "labels = sorted(df1['y_train'].unique())\n",
        "for label in range(7):\n",
        "    plot_images_per_label(df1, label, 3, (12,9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asV1O58Lrq-R"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.fromarray(X_train[0], 'RGB')\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "#df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "#df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "#df_train = df_train.drop_duplicates(subset=['lesion_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmX_Uqbmj-tN"
      },
      "outputs": [],
      "source": [
        "from numpy import moveaxis\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_el = np.inf\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 300       # how many epochs to run for\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "args['patience'] = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NydOdPMajEfT"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        # [(W−K+2P)/S]+1\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(7, stride=7),\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),# 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False), # 8\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),# 4\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 0, bias=False),#14\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**3 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        # H_out ​= (H_in​−1)*stride[0] − 2×padding[0] + dilation[0]×(kernel_size[0]−1) + output_padding[0] + 1\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4), #10\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4), #13\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h, 4),# 16\n",
        "            nn.BatchNorm2d(self.dim_h),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h, 3, 4, 2, 1),# 32\n",
        "            nn.UpsamplingBilinear2d(scale_factor=7),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**3, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def DeepSMOTE_train(X_train, y_train, one_hot = False):\n",
        "  from torch.utils.data import TensorDataset\n",
        "  import os\n",
        "\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #X_train = X_train.astype('float32') / 255.\n",
        "  \n",
        "  batch_size = args['batch_size']\n",
        "  patience = args['patience']\n",
        "  encoder = Encoder(args)\n",
        "  decoder = Decoder(args)\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  print(device)\n",
        "  decoder = decoder.to(device)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "  #decoder loss function\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  num_workers = 0\n",
        "\n",
        "  #torch.Tensor returns float so if want long then use torch.tensor\n",
        "  tensor_x = torch.from_numpy(X_train.copy())#torch.Tensor(X_train)\n",
        "  tensor_y = torch.tensor(y_train,dtype=torch.long)\n",
        "  mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "  train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "      batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "  best_loss = np.inf\n",
        "\n",
        "  enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "  dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "  for epoch in range(args['epochs']):\n",
        "      train_loss = 0.0\n",
        "      tmse_loss = 0.0\n",
        "      tdiscr_loss = 0.0\n",
        "      # train for one epoch -- set nets to train mode\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "  \n",
        "      for images,labs in train_loader:\n",
        "      \n",
        "          # zero gradients for each batch\n",
        "          encoder.zero_grad()\n",
        "          decoder.zero_grad()\n",
        "          images, labs = images.to(device), labs.to(device)\n",
        "          labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "          # run images\n",
        "          z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "      \n",
        "          x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "          mse = criterion(x_hat,images)\n",
        "                  \n",
        "          resx = []\n",
        "          resy = []\n",
        "      \n",
        "          tc = np.random.choice(num_classes,1)\n",
        "          #tc = 9\n",
        "          xbeg = X_train[y_train == tc]\n",
        "          ybeg = y_train[y_train == tc] \n",
        "          xlen = len(xbeg)\n",
        "          nsamp = min(xlen, 100)\n",
        "          ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "          xclass = xbeg[ind]\n",
        "          yclass = ybeg[ind]\n",
        "      \n",
        "          xclen = len(xclass)\n",
        "          xcminus = np.arange(1,xclen)\n",
        "          \n",
        "          xcplus = np.append(xcminus,0)\n",
        "          xcnew = (xclass[[xcplus],:])\n",
        "          xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "      \n",
        "          xcnew = torch.Tensor(xcnew)\n",
        "          xcnew = xcnew.to(device)\n",
        "      \n",
        "          #encode xclass to feature space\n",
        "          xclass = torch.Tensor(xclass)\n",
        "          xclass = xclass.to(device)\n",
        "          xclass = encoder(xclass)\n",
        "      \n",
        "          xclass = xclass.detach().cpu().numpy()\n",
        "      \n",
        "          xc_enc = (xclass[[xcplus],:])\n",
        "          xc_enc = np.squeeze(xc_enc)\n",
        "      \n",
        "          xc_enc = torch.Tensor(xc_enc)\n",
        "          xc_enc = xc_enc.to(device)\n",
        "          \n",
        "          ximg = decoder(xc_enc)\n",
        "          \n",
        "          mse2 = criterion(ximg,xcnew)\n",
        "      \n",
        "          comb_loss = mse2 + mse\n",
        "          comb_loss.backward()\n",
        "      \n",
        "          enc_optim.step()\n",
        "          dec_optim.step()\n",
        "      \n",
        "          train_loss += comb_loss.item()*images.size(0)\n",
        "          tmse_loss += mse.item()*images.size(0)\n",
        "          tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "      train_loss = train_loss/len(train_loader)\n",
        "      tmse_loss = tmse_loss/len(train_loader)\n",
        "      tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "      print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "              train_loss,tmse_loss,tdiscr_loss))\n",
        "      \n",
        "  \n",
        "  \n",
        "      #store the best encoder and decoder models\n",
        "      #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "      #necessary for illustration purposes\n",
        "      if train_loss < best_loss:\n",
        "          print('Saving..')\n",
        "          patience = args['patience']\n",
        "          path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_enc.pth'\n",
        "          path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_dec.pth'\n",
        "        \n",
        "          torch.save(encoder.state_dict(), path_enc)\n",
        "          torch.save(decoder.state_dict(), path_dec)\n",
        "  \n",
        "          best_loss = train_loss\n",
        "      else:\n",
        "          patience = patience - 1\n",
        "\n",
        "      if patience == 0:\n",
        "          print('Out of patience. \\n')\n",
        "          break\n",
        "\n",
        "def DeepSMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  batch_size = args['batch_size']\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #Generate artificial images\n",
        "  import torch\n",
        "  np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "  #path on the computer where the models are stored\n",
        "  modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/'\n",
        "\n",
        "  path_enc = modpth + '/bst_enc.pth'\n",
        "  path_dec = modpth + '/bst_dec.pth'\n",
        "  \n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  encoder = Encoder(args)\n",
        "  encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  decoder = Decoder(args)\n",
        "  decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "  decoder = decoder.to(device)\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  resx = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      #encode xclass to feature space\n",
        "      xclass = torch.Tensor(xclass)\n",
        "      xclass = xclass.to(device)\n",
        "      xclass = encoder(xclass)\n",
        "          \n",
        "      xclass = xclass.detach().cpu().numpy()\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n == 0:\n",
        "        continue\n",
        "#        resx2 = []\n",
        "#        resy2 = []\n",
        "#        for j in range(batch_size, n+batch_size+1, batch_size):\n",
        "#          if j <= n:\n",
        "#            batch_size_max = batch_size\n",
        "#          elif n % batch_size != 0:\n",
        "#            batch_size_max = n%batch_size\n",
        "#          else:\n",
        "#            break\n",
        "#          xsamp, ysamp = G_SM(xclass,yclass,batch_size_max,i)\n",
        "      xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "  \n",
        "      \"\"\"to generate samples for resnet\"\"\"   \n",
        "      xsamp = torch.Tensor(xsamp)\n",
        "      xsamp = xsamp.to(device)\n",
        "      ximg = decoder(xsamp)\n",
        "\n",
        "      ximn = ximg.detach().cpu().numpy()\n",
        "#        resx2.append(ximn)\n",
        "#        resy2.append(ysamp)\n",
        "#        \n",
        "#        resx2 = np.vstack(resx2)\n",
        "#        resy2 = np.hstack(resy2)\n",
        "      resx.append(ximn)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx1 = np.vstack(resx)\n",
        "  resy1 = np.hstack(resy)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = X_train.reshape(X_train.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "  X_train = moveaxis(X_train, 1, 3)\n",
        "  X_train = X_train * max_el\n",
        "  return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6qneWL_Bs2U"
      },
      "outputs": [],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr1jnSM7yzJc"
      },
      "outputs": [],
      "source": [
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1aAQBmiy0K"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['balanced_acc'])\n",
        "plt.plot(hst2.history['val_balanced_acc'])\n",
        "plt.title('model balance_acc after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UswA0co2y1wl",
        "iDRWiTnO0MGh",
        "eaK4zbtoaAaC",
        "3K908bbiYwbS",
        "kE8Ziq-BlEP4",
        "RcRGeofw-8tK",
        "cNBXx28B9yGu",
        "0jrJ33lUDkCM",
        "B2PgksTFkOAq"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}