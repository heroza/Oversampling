{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Oversampling/blob/main/Skin_Cancer_Diagnosis_using_Borderline-SMOTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "16de3d33-930b-4dc7-c4a9-f93dfd04f198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfcFpsBwM0d4"
      },
      "source": [
        "#Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C_s6OIGKM26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "371bd24a-4bf9-491a-e0ec-78b7eb06e6d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ff488085aaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_current_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_preprocessing_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Image preprocessing layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m \u001b[0mcython_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'core'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tk\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,AveragePooling2D,BatchNormalization,Add,ZeroPadding2D,Flatten,Dense,Input,LeakyReLU,Softmax,ReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class Attention(tk.layers.Layer):\n",
        "    \n",
        "    def __init__(self,input_channels,output_channel,kernel_size,groups):\n",
        "        super().__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channel = output_channel    \n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = 1\n",
        "        self.groups = groups\n",
        "\n",
        "        assert output_channel % groups == 0\n",
        "        \n",
        "        self.rel_h = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,kernel_size,1,output_channel//2),stddev = 0.1)) \n",
        "        #output_channels//2 is the number of channels on which the relative position will be considered,1 denotes the number of those filters and the one after that too and (kernel_size,1) denotes the size of that filter\n",
        "        self.rel_w = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,1,kernel_size,output_channel//2),stddev = 0.1)) \n",
        "\n",
        "        self.key_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.query_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.value_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "\n",
        "    def call(self,x):\n",
        "        \n",
        "        batch,height,width,channels = x.shape\n",
        "        x_padded = ZeroPadding2D(padding=(self.kernel_size//2,self.kernel_size//2))(x)\n",
        "        query = self.query_weights(x)\n",
        "        value = self.value_weights(x_padded)\n",
        "        key = self.key_weights(x_padded)\n",
        "        #key,query and value will have the shape of (batch,height,width,depth)\n",
        "        keys = tf.image.extract_patches(images = key,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        value = tf.image.extract_patches(images = value,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        no_of_kernels = key.shape[-2] - self.kernel_size + 1\n",
        "        keys = tf.reshape(keys,shape = (-1,no_of_kernels, no_of_kernels,self.kernel_size,self.kernel_size,self.output_channel))\n",
        "        key_split_h,key_split_w = tf.split(keys,num_or_size_splits = 2,axis = -1)\n",
        "        key_with_rel = tk.layers.concatenate([key_split_h + self.rel_h,key_split_w + self.rel_w],axis = -1) \n",
        "        \n",
        "        #reshaping the query and key\n",
        "        key_with_rel = tf.reshape(key_with_rel,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        query  = tf.reshape(query,(-1,self.groups,no_of_kernels,no_of_kernels,1,self.output_channel//self.groups))        \n",
        "        value = tf.reshape(value,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        \n",
        "        #multiplication  of key and query\n",
        "        #assert key_with_rel.shape == query.shape        \n",
        "        key_prod_query = query*key_with_rel\n",
        "        \n",
        "        # Now the function is passed through the softmax and is multiplied with the values\n",
        "        s = Softmax(axis = -2)(key_prod_query)\n",
        "        y = tf.einsum('bnchwk,bnchwk->bnchk',s,value)\n",
        "        y = tf.reshape(y,(-1,height,width,self.output_channel))\n",
        "        return y\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            \"input_channels\": self.input_channels, \n",
        "            \"output_channel\": self.output_channel, \n",
        "            \"kernel_size\": self.kernel_size, \n",
        "            \"stride\": self.stride, \n",
        "            \"groups\": self.groups, \n",
        "            \"rel_h\": self.rel_h, \n",
        "            \"rel_w\": self.rel_w, \n",
        "            \"key_weights\": self.key_weights, \n",
        "            \"query_weights\": self.query_weights, \n",
        "            \"value_weights\": self.value_weights\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_x4c0_DTkaa"
      },
      "source": [
        "#Library, atribut, and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR2MJBYq-oiB",
        "outputId": "9a5cca14-cf7b-4905-836c-7a50c47d5f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 224\n",
        "IMAGE_H = 224\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)\n",
        "num_classes = 7\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "opt_adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "opt_SGD = SGD(learning_rate=0.001)\n",
        "the_arch = 'resnet50'\n",
        "\n",
        "#Data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2), \n",
        "  layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),\n",
        "  layers.experimental.preprocessing.RandomTranslation(0.3, 0.3, fill_mode='reflect', interpolation='bilinear',)\n",
        "])\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model_attention.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/last_model_attention.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_balanced_acc', patience=20, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_balanced_acc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JffFid9sOXeo"
      },
      "outputs": [],
      "source": [
        "# load train and test dataset\n",
        "def preprocess_image_input(input_images, arch = the_arch):\n",
        "  input_images = input_images.astype('float32')\n",
        "  if arch == 'inception_v3':\n",
        "    output_ims = tf.keras.applications.inception_v3.preprocess_input(input_images)\n",
        "  else:\n",
        "    output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def load_cifar10_dataset():\n",
        "  from keras.datasets import cifar10\n",
        "    # load dataset\n",
        "  (X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
        "    # one hot encode target values\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_val = to_categorical(y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def balanced_acc(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "\n",
        "    tensor1 = tf.math.argmax(y_true, axis=1)\n",
        "    tensor2 = tf.math.argmax(y_pred, axis=1)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(tensor1, tensor2)\n",
        "    \n",
        "    diag = tf.linalg.tensor_diag_part (cm)\n",
        "    tpfn = tf.cast(K.sum(cm, axis = 1), tf.float32) + K.epsilon()\n",
        "    recall = tf.divide(tf.cast(diag, tf.float32),tpfn)\n",
        "    balanced_acc = K.mean(recall)\n",
        "    return balanced_acc\n",
        "\n",
        "def define_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    # compile model\n",
        "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def define_base_model(arch = the_arch, start_trainable_layer = 9999, attention=False):\n",
        "  #x = UpSampling2D(size=(7,7))(input_tensor)\n",
        "  #x = data_augmentation(input_tensor)\n",
        "  #x = layers.Rescaling(1.0 / 255)(input_tensor)  # Rescale inputs\n",
        "  if arch != 'dense':\n",
        "    input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "    if arch == 'resnet50':\n",
        "      base_model = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'inception_v3':\n",
        "      base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'ResNet':\n",
        "      base_model = ResNet(classes ,image_shape)(input_tensor)\n",
        "    x = base_model.output\n",
        "    if attention:\n",
        "      x = Attention(1024,1024,7,8)(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "    if start_trainable_layer != 9999:\n",
        "      for layer in base_model.layers[start_trainable_layer:]:\n",
        "        layer.trainable = True\n",
        "  else:\n",
        "    input_tensor = Input(shape=(2048))\n",
        "    x = input_tensor\n",
        "  #x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  #x = Dropout(0.2)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  predictions = Dense(num_classes, activation='softmax')(x)\n",
        "  model = Model(inputs=input_tensor, outputs=predictions)\n",
        "  model.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "  return model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "    # plot loss\n",
        "    plt.subplot(211)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    # plot accuracy\n",
        "    plt.subplot(212)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        " \n",
        "# scale pixels\n",
        "def norm_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm\n",
        "\n",
        "def load_isic2018_dataset(train_under_frac = 0):\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "  df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "\n",
        "  #decode one hot label\n",
        "  df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "  df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "  #random undersampling for training dataset\n",
        "  if train_under_frac !=0:\n",
        "    df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=train_under_frac).index)\n",
        "\n",
        "  #drop one-hot column\n",
        "  df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "  df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "  #make filepaths of the image\n",
        "  dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "  dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "  df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "  df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'\n",
        "  \n",
        "  #load image pixels to dataframe\n",
        "  df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "  df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, df_train, df_val\n",
        "\n",
        "def reset_dataset(df_train, df_val):\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  X_train = preprocess_image_input(X_train, the_arch)\n",
        "  X_val = preprocess_image_input(X_val, the_arch)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def SMOTE_Data(X, y, one_hot = False, k = 5, width = IMAGE_W, height = IMAGE_H, c = 3, type = 'smote'):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  if type == 'borderline':\n",
        "    sm = BorderlineSMOTE(random_state=42, k_neighbors=k)\n",
        "  elif type == 'svm':\n",
        "    sm = SVMSMOTE()\n",
        "  elif type == 'adasyn':\n",
        "    sm = ADASYN(random_state=42, n_neighbors=k)\n",
        "  elif type == 'kmeans':\n",
        "    sm = KMeansSMOTE(k_neighbors=k, kmeans_estimator=10)\n",
        "  else:\n",
        "    sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, width * height * c)), y)\n",
        "  X_resampled = X_resampled.reshape(-1, width, height, c)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled\n",
        "\n",
        "def SMOTE_Data2(X, y, one_hot = False, k = 5):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswA0co2y1wl"
      },
      "source": [
        "#Exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnqJYIONy34l"
      },
      "outputs": [],
      "source": [
        "input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "base_model = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model(input_tensor, training=False)\n",
        "x = Attention(2048,2048,7,8)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "res50 = Model(inputs=input_tensor, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcn8hQg3J8yP"
      },
      "outputs": [],
      "source": [
        "#Train i-last layer\n",
        "# summarize feature map shapes\n",
        "for i in range(len(model.layers)):\n",
        "    layer = model.layers[i]\n",
        "    # summarize output shape\n",
        "    print(i, layer.name, layer.output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA7Af2Y73FUv"
      },
      "outputs": [],
      "source": [
        "X_train = res50.predict(X_train)\n",
        "X_val = res50.predict(X_val)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krJiAb1m3QNf"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = SMOTE_Data2(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qge6cnxQPnH6",
        "outputId": "19b126b3-07ff-45dd-d338-6075f45ac935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5321, 224, 224, 3)\n",
            "(5321, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 1113, 2: 1099, 1: 514, 0: 327, 6: 142, 3: 115})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "df1 = pd.read_pickle(path+\"isic2018_train.pkl\")\n",
        "X_train = df1.loc[:, df1.columns != 'y_train'].to_numpy()\n",
        "X_train = X_train.reshape(-1,224,224,3)\n",
        "y_train = df1.loc[:, df1.columns == 'y_train'].to_numpy()\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "df1 = pd.read_pickle(path+\"isic2018_val.pkl\")\n",
        "X_val = df1.loc[:, df1.columns != 'y_val'].to_numpy()\n",
        "X_val = X_val.reshape(-1,224,224,3)\n",
        "y_val = df1.loc[:, df1.columns == 'y_val'].to_numpy()\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArGWuciBt_-",
        "outputId": "3047fa09-1847-4e56-9481-2c58d7fa3dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14077, 224, 224, 3)\n",
            "(14077, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 2011, 2: 2011, 3: 2011, 0: 2011, 1: 2011, 6: 2011})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train, True, type='borderline')\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7Z_nccu6QjB"
      },
      "outputs": [],
      "source": [
        "#path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "#df1 = pd.DataFrame(X_train.reshape(X_train.shape[0],-1))\n",
        "#df1['y_train'] = np.argmax(y_train, axis=1).tolist()\n",
        "#df2 = pd.DataFrame(X_val.reshape(X_val.shape[0],-1))\n",
        "#df2['y_val'] = np.argmax(y_val, axis=1).tolist()\n",
        "#df1.to_pickle(path+\"isic2018_train.pkl\")\n",
        "#df2.to_pickle(path+\"isic2018_val.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMBgWqIsAAB"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vIygrW81Ln4z",
        "outputId": "0f0574c0-7dd7-48b6-94c6-9cac3ea983ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.5157 - accuracy: 0.4549 - balanced_acc: 0.4548\n",
            "Epoch 1: val_balanced_acc improved from -inf to 0.23714, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 57s 207ms/step - loss: 1.5157 - accuracy: 0.4549 - balanced_acc: 0.4548 - val_loss: 1.3714 - val_accuracy: 0.4560 - val_balanced_acc: 0.2371 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1271 - accuracy: 0.6110 - balanced_acc: 0.6108\n",
            "Epoch 2: val_balanced_acc improved from 0.23714 to 0.24518, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 45s 199ms/step - loss: 1.1271 - accuracy: 0.6110 - balanced_acc: 0.6108 - val_loss: 1.1404 - val_accuracy: 0.5596 - val_balanced_acc: 0.2452 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9838 - accuracy: 0.6614 - balanced_acc: 0.6633\n",
            "Epoch 3: val_balanced_acc improved from 0.24518 to 0.29825, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 45s 203ms/step - loss: 0.9838 - accuracy: 0.6614 - balanced_acc: 0.6633 - val_loss: 1.0647 - val_accuracy: 0.6114 - val_balanced_acc: 0.2983 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8995 - accuracy: 0.6901 - balanced_acc: 0.6905\n",
            "Epoch 4: val_balanced_acc improved from 0.29825 to 0.31016, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 44s 202ms/step - loss: 0.8995 - accuracy: 0.6901 - balanced_acc: 0.6905 - val_loss: 0.9812 - val_accuracy: 0.6528 - val_balanced_acc: 0.3102 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8427 - accuracy: 0.7076 - balanced_acc: 0.7067\n",
            "Epoch 5: val_balanced_acc improved from 0.31016 to 0.36076, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 44s 202ms/step - loss: 0.8427 - accuracy: 0.7076 - balanced_acc: 0.7067 - val_loss: 0.9410 - val_accuracy: 0.6736 - val_balanced_acc: 0.3608 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7952 - accuracy: 0.7249 - balanced_acc: 0.7235\n",
            "Epoch 6: val_balanced_acc improved from 0.36076 to 0.37016, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 44s 201ms/step - loss: 0.7952 - accuracy: 0.7249 - balanced_acc: 0.7235 - val_loss: 0.9424 - val_accuracy: 0.6788 - val_balanced_acc: 0.3702 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7635 - accuracy: 0.7379 - balanced_acc: 0.7371\n",
            "Epoch 7: val_balanced_acc improved from 0.37016 to 0.38408, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 44s 202ms/step - loss: 0.7635 - accuracy: 0.7379 - balanced_acc: 0.7371 - val_loss: 0.8860 - val_accuracy: 0.6995 - val_balanced_acc: 0.3841 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7317 - accuracy: 0.7480 - balanced_acc: 0.7497\n",
            "Epoch 8: val_balanced_acc did not improve from 0.38408\n",
            "219/219 [==============================] - 43s 197ms/step - loss: 0.7317 - accuracy: 0.7480 - balanced_acc: 0.7497 - val_loss: 0.8985 - val_accuracy: 0.6943 - val_balanced_acc: 0.3825 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7049 - accuracy: 0.7579 - balanced_acc: 0.7567\n",
            "Epoch 9: val_balanced_acc improved from 0.38408 to 0.38855, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 44s 203ms/step - loss: 0.7049 - accuracy: 0.7579 - balanced_acc: 0.7567 - val_loss: 0.8845 - val_accuracy: 0.7047 - val_balanced_acc: 0.3885 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6856 - accuracy: 0.7644 - balanced_acc: 0.7633\n",
            "Epoch 10: val_balanced_acc improved from 0.38855 to 0.39067, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 44s 202ms/step - loss: 0.6856 - accuracy: 0.7644 - balanced_acc: 0.7633 - val_loss: 0.8298 - val_accuracy: 0.7150 - val_balanced_acc: 0.3907 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.7703 - balanced_acc: 0.7704\n",
            "Epoch 11: val_balanced_acc improved from 0.39067 to 0.41682, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 45s 203ms/step - loss: 0.6676 - accuracy: 0.7703 - balanced_acc: 0.7704 - val_loss: 0.8328 - val_accuracy: 0.7150 - val_balanced_acc: 0.4168 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.7797 - balanced_acc: 0.7799\n",
            "Epoch 12: val_balanced_acc did not improve from 0.41682\n",
            "219/219 [==============================] - 43s 195ms/step - loss: 0.6474 - accuracy: 0.7797 - balanced_acc: 0.7799 - val_loss: 0.8640 - val_accuracy: 0.6995 - val_balanced_acc: 0.3824 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.7851 - balanced_acc: 0.7850\n",
            "Epoch 13: val_balanced_acc did not improve from 0.41682\n",
            "219/219 [==============================] - 43s 195ms/step - loss: 0.6305 - accuracy: 0.7851 - balanced_acc: 0.7850 - val_loss: 0.8192 - val_accuracy: 0.7047 - val_balanced_acc: 0.3847 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.7877 - balanced_acc: 0.7873\n",
            "Epoch 14: val_balanced_acc did not improve from 0.41682\n",
            "219/219 [==============================] - 43s 196ms/step - loss: 0.6188 - accuracy: 0.7877 - balanced_acc: 0.7873 - val_loss: 0.8619 - val_accuracy: 0.6943 - val_balanced_acc: 0.3788 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5968 - accuracy: 0.7956 - balanced_acc: 0.7974\n",
            "Epoch 15: val_balanced_acc did not improve from 0.41682\n",
            "219/219 [==============================] - 43s 195ms/step - loss: 0.5968 - accuracy: 0.7956 - balanced_acc: 0.7974 - val_loss: 0.8053 - val_accuracy: 0.7098 - val_balanced_acc: 0.3926 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5914 - accuracy: 0.8008 - balanced_acc: 0.8003\n",
            "Epoch 16: val_balanced_acc did not improve from 0.41682\n",
            "219/219 [==============================] - 43s 196ms/step - loss: 0.5914 - accuracy: 0.8008 - balanced_acc: 0.8003 - val_loss: 0.8210 - val_accuracy: 0.6995 - val_balanced_acc: 0.3790 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.8040 - balanced_acc: 0.8037\n",
            "Epoch 17: val_balanced_acc did not improve from 0.41682\n",
            "219/219 [==============================] - 43s 195ms/step - loss: 0.5806 - accuracy: 0.8040 - balanced_acc: 0.8037 - val_loss: 0.8263 - val_accuracy: 0.7098 - val_balanced_acc: 0.3910 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5638 - accuracy: 0.8115 - balanced_acc: 0.8101\n",
            "Epoch 18: val_balanced_acc did not improve from 0.41682\n",
            "219/219 [==============================] - 43s 196ms/step - loss: 0.5638 - accuracy: 0.8115 - balanced_acc: 0.8101 - val_loss: 0.7997 - val_accuracy: 0.7098 - val_balanced_acc: 0.3868 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5567 - accuracy: 0.8142 - balanced_acc: 0.8116\n",
            "Epoch 19: val_balanced_acc did not improve from 0.41682\n",
            "219/219 [==============================] - 43s 196ms/step - loss: 0.5567 - accuracy: 0.8142 - balanced_acc: 0.8116 - val_loss: 0.8018 - val_accuracy: 0.7047 - val_balanced_acc: 0.3890 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.8160 - balanced_acc: 0.8147\n",
            "Epoch 20: val_balanced_acc did not improve from 0.41682\n",
            "219/219 [==============================] - 43s 196ms/step - loss: 0.5440 - accuracy: 0.8160 - balanced_acc: 0.8147 - val_loss: 0.7591 - val_accuracy: 0.7150 - val_balanced_acc: 0.3891 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# define model\n",
        "model = define_base_model('resnet50')\n",
        "#model.summary()\n",
        "hst = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n",
        "# learning curves\n",
        "summarize_diagnostics(hst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vXnW3lmCgln3",
        "outputId": "ea9ac4c9-6256-4ee6-a587-c5045e3ef6d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwwLiXUSG0IZ"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "#hst = model.fit(train_data_batches,\n",
        "#                    epochs = EPOCHS, validation_data = valid_data_batches,      \n",
        "                    #steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "#                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgjmi-4UIT-"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SPz8NH1Oylv9"
      },
      "outputs": [],
      "source": [
        "#save last model\n",
        "model.save(last_model_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS3ewyxO_anU",
        "outputId": "9d348c5c-97c0-41c3-d7e4-9383c50b0935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.8216949634155004\n",
            "balanced accuracy on training 0.8216949634155004\n",
            "accuracy on validation 0.7150259067357513\n",
            "balanced accuracy on validation 0.5538878078947765\n",
            "Score on val data:  (0.464630423154622, 0.5538878078947765, 0.4972932330827068, None)\n"
          ]
        }
      ],
      "source": [
        "last_model = load_model(last_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = last_model.predict(X_train)\n",
        "y_val_pred = last_model.predict(X_val)\n",
        "\n",
        "#print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3IyWjdGG4Xq",
        "outputId": "a0c2e6a7-f8a6-4802-98dc-9dec702cdfd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.7762307309796121\n",
            "balanced accuracy on training 0.7762307309796121\n",
            "accuracy on validation 0.7150259067357513\n",
            "balanced accuracy on validation 0.5425241715311401\n",
            "Score on val data:  (0.47033836832223924, 0.5425241715311401, 0.49718272162832566, None)\n"
          ]
        }
      ],
      "source": [
        "best_model = load_model(best_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRWiTnO0MGh"
      },
      "source": [
        "#Cut-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGnCoIdLyDHS"
      },
      "outputs": [],
      "source": [
        "df_val_pred = pd.DataFrame(y_val_pred, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdyCbloQyWTC"
      },
      "outputs": [],
      "source": [
        "numbers = [float(x)/40 for x in range(11)]\n",
        "for i in numbers:\n",
        "    df_val_pred[i]= df_val_pred.MEL.map(lambda x: 1 if x > i else 0)\n",
        "df_val_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4SQsRx73kgk"
      },
      "outputs": [],
      "source": [
        "y_val_true= [1 if x == 4 else 0 for x in np.argmax(y_val, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcUISWFi0J05"
      },
      "outputs": [],
      "source": [
        "#num = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
        "cutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n",
        "for i in numbers:\n",
        "    cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "    total1=sum(sum(cm1))\n",
        "    Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31LSzov1tCt"
      },
      "outputs": [],
      "source": [
        "cutoff_df[['Accuracy','Sensitivity','Specificity']].plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CIKT94Jqye"
      },
      "outputs": [],
      "source": [
        "i = 0.025\n",
        "cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "total1=sum(sum(cm1))\n",
        "Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U2tkFebL_VC"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ', Accuracy)\n",
        "print('Sensitivity: ', Sensitivity)\n",
        "print('Specificity: ', Specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkPOFLehOmFg"
      },
      "outputs": [],
      "source": [
        "#change melanoma flag back to 4\n",
        "df_val_pred[df_val_pred[i] == 1] = 4\n",
        "#decode one-hot y_val_pred while use cut-off melanoma data\n",
        "condition = df_val_pred[i] == 4\n",
        "y_val_pred2 = np.where(condition, df_val_pred[i], np.argmax(y_val_pred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOVl6dWlTDLo"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ',accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))\n",
        "print('Balanced accuracy: ',balanced_accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvYutTKRhR_"
      },
      "outputs": [],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_val, axis=1), y_val_pred2)\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtvW3YeaLlC"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-1yjWGeKs7"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "#labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "#f = plt.figure(figsize=(15, 6))\n",
        "#s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "#s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic95mefkpG3"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "df_test['image_px'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60LYAT7VsNOZ"
      },
      "outputs": [],
      "source": [
        "X_test = np.asarray(df_test['image_px'].tolist())\n",
        "print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXnnIIwC4cHE"
      },
      "outputs": [],
      "source": [
        "#preprocess\n",
        "X_test = preprocess_image_input(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeDTXdaMLmyU"
      },
      "outputs": [],
      "source": [
        "#X_test = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIX0AmEFNv3Y"
      },
      "outputs": [],
      "source": [
        "# predicting\n",
        "#CHANGE THE MODEL IF NECESSARY\n",
        "#X_test2 = model1.predict(X_test)\n",
        "Y_pred2 = model.predict(X_test2)\n",
        "print(\"Y_pred2\", Y_pred2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]\n",
        "df_pred.set_index(\"image\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ynyd8PjT589"
      },
      "outputs": [],
      "source": [
        "#update MEL data using cut-off value\n",
        "df_pred.MEL[df_pred.MEL > i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjRdONoQVMq0"
      },
      "outputs": [],
      "source": [
        "df_pred.loc[df_pred.MEL > i, ['NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_SMOTE_Attention.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0MghVs0tsGw"
      },
      "source": [
        "result: 0.656"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE8Ziq-BlEP4"
      },
      "source": [
        "#Oversampling on feature map level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtgmvyhCndpB"
      },
      "outputs": [],
      "source": [
        "i = 176"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm05Zet_B5am"
      },
      "outputs": [],
      "source": [
        "for i in range(len(model.layers)):\n",
        "  layer = model.layers[i]\n",
        "  print(i, layer.name, layer.output_shape, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqeSic6NmLsR"
      },
      "outputs": [],
      "source": [
        "# redefine model to output right after the first hidden layer\n",
        "model1 = Model(inputs=model.inputs, outputs=model.layers[i].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVHYG9Rwm28i"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "X_train_fm = model1.predict(X_train)\n",
        "X_val_fm = model1.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNozN8-wDUNL"
      },
      "outputs": [],
      "source": [
        "X_train_fm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19hK7aQNeAQo"
      },
      "outputs": [],
      "source": [
        "X_train_fm_ov, y_train_ov = SMOTE_Data2(X_train_fm, y_train, True, 5)\n",
        "print(X_train_fm_ov.shape)\n",
        "print(y_train_ov.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train_ov, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qP4iyYcnAYa"
      },
      "outputs": [],
      "source": [
        "model2 = Model(inputs=model.layers[i].output, outputs=model.layers[len(model.layers)-1].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzdjs0WbvDB0"
      },
      "outputs": [],
      "source": [
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/last_model_no.h5'\n",
        "model2.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst = model2.fit(X_train_fm_ov, y_train_ov, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val_fm, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train_fm_ov.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XhlbWn--8Or"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW-_U6vFpIci"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "y_train_pred = model2.predict(X_train_fm_ov)\n",
        "y_val_pred = model2.predict(X_val_fm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLop0YK-ZK40"
      },
      "outputs": [],
      "source": [
        "print('accuracy on training',accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load ISIC 2018 Challange Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3P7IjyLuZGY"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_isic2018_dataset(train_under_frac = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IncA-_o_n5w"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = y_train.value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(y_train == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "labels = sorted(df1['y_train'].unique())\n",
        "for label in range(7):\n",
        "    plot_images_per_label(df1, label, 3, (12,9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asV1O58Lrq-R"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.fromarray(X_train[0], 'RGB')\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "#df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "#df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "#df_train = df_train.drop_duplicates(subset=['lesion_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmX_Uqbmj-tN"
      },
      "outputs": [],
      "source": [
        "from numpy import moveaxis\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_el = np.inf\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 300       # how many epochs to run for\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "args['patience'] = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NydOdPMajEfT"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        # [(WK+2P)/S]+1\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(7, stride=7),\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),# 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False), # 8\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),# 4\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 0, bias=False),#14\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**3 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        # H_out = (H_in1)*stride[0]  2padding[0] + dilation[0](kernel_size[0]1) + output_padding[0] + 1\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4), #10\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4), #13\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h, 4),# 16\n",
        "            nn.BatchNorm2d(self.dim_h),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h, 3, 4, 2, 1),# 32\n",
        "            nn.UpsamplingBilinear2d(scale_factor=7),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**3, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def DeepSMOTE_train(X_train, y_train, one_hot = False):\n",
        "  from torch.utils.data import TensorDataset\n",
        "  import os\n",
        "\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #X_train = X_train.astype('float32') / 255.\n",
        "  \n",
        "  batch_size = args['batch_size']\n",
        "  patience = args['patience']\n",
        "  encoder = Encoder(args)\n",
        "  decoder = Decoder(args)\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  print(device)\n",
        "  decoder = decoder.to(device)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "  #decoder loss function\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  num_workers = 0\n",
        "\n",
        "  #torch.Tensor returns float so if want long then use torch.tensor\n",
        "  tensor_x = torch.from_numpy(X_train.copy())#torch.Tensor(X_train)\n",
        "  tensor_y = torch.tensor(y_train,dtype=torch.long)\n",
        "  mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "  train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "      batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "  best_loss = np.inf\n",
        "\n",
        "  enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "  dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "  for epoch in range(args['epochs']):\n",
        "      train_loss = 0.0\n",
        "      tmse_loss = 0.0\n",
        "      tdiscr_loss = 0.0\n",
        "      # train for one epoch -- set nets to train mode\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "  \n",
        "      for images,labs in train_loader:\n",
        "      \n",
        "          # zero gradients for each batch\n",
        "          encoder.zero_grad()\n",
        "          decoder.zero_grad()\n",
        "          images, labs = images.to(device), labs.to(device)\n",
        "          labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "          # run images\n",
        "          z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "      \n",
        "          x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "          mse = criterion(x_hat,images)\n",
        "                  \n",
        "          resx = []\n",
        "          resy = []\n",
        "      \n",
        "          tc = np.random.choice(num_classes,1)\n",
        "          #tc = 9\n",
        "          xbeg = X_train[y_train == tc]\n",
        "          ybeg = y_train[y_train == tc] \n",
        "          xlen = len(xbeg)\n",
        "          nsamp = min(xlen, 100)\n",
        "          ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "          xclass = xbeg[ind]\n",
        "          yclass = ybeg[ind]\n",
        "      \n",
        "          xclen = len(xclass)\n",
        "          xcminus = np.arange(1,xclen)\n",
        "          \n",
        "          xcplus = np.append(xcminus,0)\n",
        "          xcnew = (xclass[[xcplus],:])\n",
        "          xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "      \n",
        "          xcnew = torch.Tensor(xcnew)\n",
        "          xcnew = xcnew.to(device)\n",
        "      \n",
        "          #encode xclass to feature space\n",
        "          xclass = torch.Tensor(xclass)\n",
        "          xclass = xclass.to(device)\n",
        "          xclass = encoder(xclass)\n",
        "      \n",
        "          xclass = xclass.detach().cpu().numpy()\n",
        "      \n",
        "          xc_enc = (xclass[[xcplus],:])\n",
        "          xc_enc = np.squeeze(xc_enc)\n",
        "      \n",
        "          xc_enc = torch.Tensor(xc_enc)\n",
        "          xc_enc = xc_enc.to(device)\n",
        "          \n",
        "          ximg = decoder(xc_enc)\n",
        "          \n",
        "          mse2 = criterion(ximg,xcnew)\n",
        "      \n",
        "          comb_loss = mse2 + mse\n",
        "          comb_loss.backward()\n",
        "      \n",
        "          enc_optim.step()\n",
        "          dec_optim.step()\n",
        "      \n",
        "          train_loss += comb_loss.item()*images.size(0)\n",
        "          tmse_loss += mse.item()*images.size(0)\n",
        "          tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "      train_loss = train_loss/len(train_loader)\n",
        "      tmse_loss = tmse_loss/len(train_loader)\n",
        "      tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "      print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "              train_loss,tmse_loss,tdiscr_loss))\n",
        "      \n",
        "  \n",
        "  \n",
        "      #store the best encoder and decoder models\n",
        "      #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "      #necessary for illustration purposes\n",
        "      if train_loss < best_loss:\n",
        "          print('Saving..')\n",
        "          patience = args['patience']\n",
        "          path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_enc.pth'\n",
        "          path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_dec.pth'\n",
        "        \n",
        "          torch.save(encoder.state_dict(), path_enc)\n",
        "          torch.save(decoder.state_dict(), path_dec)\n",
        "  \n",
        "          best_loss = train_loss\n",
        "      else:\n",
        "          patience = patience - 1\n",
        "\n",
        "      if patience == 0:\n",
        "          print('Out of patience. \\n')\n",
        "          break\n",
        "\n",
        "def DeepSMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  batch_size = args['batch_size']\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #Generate artificial images\n",
        "  import torch\n",
        "  np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "  #path on the computer where the models are stored\n",
        "  modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/'\n",
        "\n",
        "  path_enc = modpth + '/bst_enc.pth'\n",
        "  path_dec = modpth + '/bst_dec.pth'\n",
        "  \n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  encoder = Encoder(args)\n",
        "  encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  decoder = Decoder(args)\n",
        "  decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "  decoder = decoder.to(device)\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  resx = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      #encode xclass to feature space\n",
        "      xclass = torch.Tensor(xclass)\n",
        "      xclass = xclass.to(device)\n",
        "      xclass = encoder(xclass)\n",
        "          \n",
        "      xclass = xclass.detach().cpu().numpy()\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n == 0:\n",
        "        continue\n",
        "#        resx2 = []\n",
        "#        resy2 = []\n",
        "#        for j in range(batch_size, n+batch_size+1, batch_size):\n",
        "#          if j <= n:\n",
        "#            batch_size_max = batch_size\n",
        "#          elif n % batch_size != 0:\n",
        "#            batch_size_max = n%batch_size\n",
        "#          else:\n",
        "#            break\n",
        "#          xsamp, ysamp = G_SM(xclass,yclass,batch_size_max,i)\n",
        "      xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "  \n",
        "      \"\"\"to generate samples for resnet\"\"\"   \n",
        "      xsamp = torch.Tensor(xsamp)\n",
        "      xsamp = xsamp.to(device)\n",
        "      ximg = decoder(xsamp)\n",
        "\n",
        "      ximn = ximg.detach().cpu().numpy()\n",
        "#        resx2.append(ximn)\n",
        "#        resy2.append(ysamp)\n",
        "#        \n",
        "#        resx2 = np.vstack(resx2)\n",
        "#        resy2 = np.hstack(resy2)\n",
        "      resx.append(ximn)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx1 = np.vstack(resx)\n",
        "  resy1 = np.hstack(resy)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = X_train.reshape(X_train.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "  X_train = moveaxis(X_train, 1, 3)\n",
        "  X_train = X_train * max_el\n",
        "  return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6qneWL_Bs2U"
      },
      "outputs": [],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr1jnSM7yzJc"
      },
      "outputs": [],
      "source": [
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1aAQBmiy0K"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['balanced_acc'])\n",
        "plt.plot(hst2.history['val_balanced_acc'])\n",
        "plt.title('model balance_acc after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UswA0co2y1wl",
        "iDRWiTnO0MGh",
        "eaK4zbtoaAaC",
        "3K908bbiYwbS",
        "kE8Ziq-BlEP4",
        "RcRGeofw-8tK",
        "cNBXx28B9yGu",
        "0jrJ33lUDkCM",
        "B2PgksTFkOAq"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}